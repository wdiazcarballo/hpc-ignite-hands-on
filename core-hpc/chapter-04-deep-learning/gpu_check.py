#!/usr/bin/env python3
"""
GPU Check - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU ‡πÅ‡∏•‡∏∞ CUDA
Chapter 4: Deep Learning on HPC

‡∏£‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô training ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤ GPU ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
"""

import sys


def check_gpu():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU availability ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""

    print("=" * 60)
    print("üîç GPU and CUDA Check")
    print("   Chapter 4: Deep Learning on HPC")
    print("=" * 60)

    # Check PyTorch
    try:
        import torch
        print(f"\n‚úÖ PyTorch version: {torch.__version__}")
    except ImportError:
        print("\n‚ùå PyTorch not installed!")
        sys.exit(1)

    # Check CUDA
    cuda_available = torch.cuda.is_available()
    print(f"\nüìä CUDA Status:")
    print(f"   CUDA available: {cuda_available}")

    if cuda_available:
        print(f"   CUDA version: {torch.version.cuda}")
        print(f"   cuDNN version: {torch.backends.cudnn.version()}")

        # GPU count
        gpu_count = torch.cuda.device_count()
        print(f"\nüéÆ GPU Count: {gpu_count}")

        # GPU details
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            print(f"\n   GPU {i}: {props.name}")
            print(f"   - Compute capability: {props.major}.{props.minor}")
            print(f"   - Total memory: {props.total_memory / 1024**3:.1f} GB")
            print(f"   - Multi-processor count: {props.multi_processor_count}")

        # Current device
        print(f"\nüìç Current device: {torch.cuda.current_device()}")
        print(f"   Device name: {torch.cuda.get_device_name()}")

        # Memory info
        print(f"\nüíæ Memory (GPU 0):")
        print(f"   Allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
        print(f"   Cached: {torch.cuda.memory_reserved() / 1024**2:.1f} MB")

        # Simple test
        print("\nüß™ Running simple GPU test...")
        try:
            x = torch.randn(1000, 1000, device="cuda")
            y = torch.mm(x, x)
            del x, y
            torch.cuda.empty_cache()
            print("   ‚úÖ GPU computation successful!")
        except Exception as e:
            print(f"   ‚ùå GPU test failed: {e}")

    else:
        print("\n‚ö†Ô∏è CUDA not available. Running on CPU only.")
        print("   For GPU support on LANTA:")
        print("   module load PyTorch/2.0.1-CUDA-11.7.0")

    # Check for MPS (Apple Silicon)
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        print("\nüçé Apple MPS available (Metal Performance Shaders)")

    print("\n" + "=" * 60)
    print("‚úÖ GPU check complete!")
    print("=" * 60)

    return cuda_available


if __name__ == "__main__":
    check_gpu()
